{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from rake_nltk import Rake\n",
    "import networkx\n",
    "# 一个图结构的相关操作包，没用过无所谓，有兴趣可以搜索学习\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import yake\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(text):\n",
    "    # 格式化输出，缩进4个单位\n",
    "    print(json.dumps(text, sort_keys = True, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_dir(dirname):\n",
    "    # 文档内容集合\n",
    "    text = []\n",
    "    # 遍历文档\n",
    "    for root, dirs, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            # 获取文件名\n",
    "            filename = os.path.splitext(file)[0]\n",
    "            # 读取文件\n",
    "            content = json.load(open(root + '/' + file, 'r', encoding = 'utf-8-sig'))\n",
    "            # 文件名添加到文件中，方便后续生成中间件\n",
    "            content['FileName'] = filename\n",
    "            text.append(content)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_extraction(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text['Text'])\n",
    "    json_print(text['Text'])\n",
    "    print(\"==============================\")\n",
    "    print(r.get_ranked_phrases())\n",
    "    print(\"==============================\")\n",
    "    print(r.get_ranked_phrases_with_scores())\n",
    "    print(\"==============================\")\n",
    "    print(r.stopwords)\n",
    "    print(\"==============================\")\n",
    "    print(r.get_word_degrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_it(text):\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 20\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    print(r.get_word_degrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_it(text):\n",
    "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    return model.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words = 'english', use_maxsum=True, nr_candidates=20, top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('president donald trump', 0.0001618786410693896)\n",
      "('health', 0.00018860144425106532)\n",
      "('president trump', 0.000256140016095592)\n",
      "('coronavirus cases', 0.000290496502101756)\n",
      "('york city health', 0.00030478933814995)\n",
      "('coronavirus', 0.0003320549968023795)\n",
      "('state health department', 0.0003391672858938273)\n",
      "('york city', 0.00034863000497000683)\n",
      "('tulsa health department', 0.00041352949780875656)\n",
      "('world health organization', 0.00044023399925992036)\n",
      "('city health department', 0.00044367872355489984)\n",
      "('cases', 0.0004612491174696666)\n",
      "('health department', 0.0004675250162042261)\n",
      "('york city mayor', 0.0004956198759993473)\n",
      "('united states', 0.0005139852510859225)\n",
      "('state health officials', 0.0005147916150894423)\n",
      "('white house coronavirus', 0.0005865249150822455)\n",
      "('johns hopkins university', 0.0006120157458651665)\n",
      "('state health', 0.0006151640445055714)\n",
      "('china national health', 0.0006281106730667024)\n"
     ]
    }
   ],
   "source": [
    "sentences = ''\n",
    "sentences += text[2]['Text']\n",
    "sentences += text[7]['Text']\n",
    "sentences += text[12]['Text']\n",
    "sentences += text[19]['Text']\n",
    "yake_it(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词向量\n",
    "# 该词向量文件形式为：词 空格 词向量，然后换行\n",
    "# 从http://nlp.stanford.edu/data/glove.6B.zip获取GloVe\n",
    "word_embeddings = {}\n",
    "GLOVE_DIR = 'glove.6B.100d.txt'\n",
    "with open(GLOVE_DIR,encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data_from_dir('news_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eywords_extraction(text[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 China says to have ‘prudent’ oil exploration with Philippines – Manila Bulletin\n",
      "1 PH to build more cellular sites on Pagasa Island after officials receive ‘welcome’ text messages from China, Vietnam – Manila Bulletin\n",
      "2 March 30 coronavirus news\n",
      "3 Malaysia power shift hits China infrastructure drive – Manila Bulletin\n",
      "4 5 things to know for June 8: George Floyd, police reform, coronavirus, economy, China\n",
      "5 Why this Japan-China island dispute could be Asia's next military flashpoint\n",
      "6 February 4 coronavirus news\n",
      "7 The coronavirus has grounded Chinese tourists. The global travel industry may not recover for years\n",
      "8 Beijing's crackdown in Xinjiang has separated thousands of children from their parents, new report claims. CNN found two of them\n",
      "9 The latest on the coronavirus pandemic and vaccines\n",
      "10 ‘Abominable’ film axed in Malaysia after rebuffing order to cut China map – Manila Bulletin\n",
      "11 Family of Thai immigrant, 84, says fatal attack 'was driven by hate'\n",
      "12 The coronavirus pandemic began in China. Today, it reported no new local infections for the first time\n",
      "13 Malaysia to be firmer in row over South China Sea – Manila Bulletin\n",
      "14 Why Thailand isn't reopening to international tourists yet\n",
      "15 Philippines mulls tourists for Thitu, bolstering South China Sea claims – Manila Bulletin\n",
      "16 The syrupy treat that helped China's Manchu Army conquer the Ming Dynasty\n",
      "17 Jo Shelley\n",
      "18 Hong Kong protests over China extradition bill\n",
      "19 June 17 coronavirus news\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i, text[i]['Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['## health ## ## travel ## ## daily life ## new york city',\n",
       " '## daily life ## ## business ## around 14 million south korean households',\n",
       " '## health ## cnn chief white house correspondent jim acosta asked president trump',\n",
       " '## health ## new york city fire department paramedic anthony almojera said',\n",
       " '## daily life ## ## travel ## president trump raised questions',\n",
       " 'weeks .” junior health minister helen whately told bbc radio',\n",
       " '## daily life ## michigan governor gretchen whitmer told cnn',\n",
       " '## business ## new york city mayor bill de blasio said',\n",
       " '.” ## health ## new york city mayor bill de blasio',\n",
       " 'simple process .” ## business ## us markets opened solidly']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "# 断句，并写入sentences列表\n",
    "sentences.append(get_rank_phrases(text[2]['Text']))\n",
    "sentences.append(get_rank_phrases(text[7]['Text']))\n",
    "sentences.append(get_rank_phrases(text[12]['Text']))\n",
    "sentences.append(get_rank_phrases(text[19]['Text']))\n",
    "# 原数据是好几篇文章，本代码将所有文章的所有句子放在一个列表里，摘要抽取也是基于所有句子（文章）的。\n",
    "sentences = [y for x in sentences for y in x]\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   health       travel       daily life    new york city',\n",
       " '   daily life       business    around    million south korean households',\n",
       " '   health    cnn chief white house correspondent jim acosta asked president trump',\n",
       " '   health    new york city fire department paramedic anthony almojera said',\n",
       " '   daily life       travel    president trump raised questions',\n",
       " 'weeks    junior health minister helen whately told bbc radio',\n",
       " '   daily life    michigan governor gretchen whitmer told cnn',\n",
       " '   business    new york city mayor bill de blasio said',\n",
       " '      health    new york city mayor bill de blasio',\n",
       " 'simple process       business    us markets opened solidly']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本清洗，去除标点、数字、特殊符号、统一小写\n",
    "clean_sentences = pd.Series(sentences).str.replace('[^a-zA-Z]', ' ')\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "clean_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['health travel daily life new york city',\n",
       " 'daily life business around million south korean households',\n",
       " 'health cnn chief white house correspondent jim acosta asked president trump',\n",
       " 'health new york city fire department paramedic anthony almojera said',\n",
       " 'daily life travel president trump raised questions',\n",
       " 'weeks junior health minister helen whately told bbc radio',\n",
       " 'daily life michigan governor gretchen whitmer told cnn',\n",
       " 'business new york city mayor bill de blasio said',\n",
       " 'health new york city mayor bill de blasio',\n",
       " 'simple process business us markets opened solidly']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(str):\n",
    "    sen = ' '.join([i for i in str if i not in stop_words])\n",
    "    return sen\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "clean_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9028, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取每个句子的所有组成词的向量（从GloVe词向量文件中获取，每个向量大小为100），\n",
    "# 然后取这些向量的平均值，得出这个句子的合并向量为这个句子的特征向量\n",
    "sentences_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum(\n",
    "            [word_embeddings.get(w, np.zeros((100,))) for w in i.split()]\n",
    "        )/(len(i.split()) + 1e-2)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentences_vectors.append(v)\n",
    "len(sentences_vectors), len(sentences_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似度矩阵，基于余弦相似度\n",
    "similarity_matrix = np.zeros((len(clean_sentences),len(clean_sentences)))\n",
    "# 初始化相似度矩阵（全零矩阵）\n",
    "for i in range(len(clean_sentences)):\n",
    "    for j in range(len(clean_sentences)):\n",
    "        if i != j:\n",
    "            similarity_matrix[i][j] = cosine_similarity(\n",
    "                sentences_vectors[i].reshape(1,-1), sentences_vectors[j].reshape(1,-1)\n",
    "            )\n",
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排序\n",
    "nx_graph = networkx.from_numpy_array(similarity_matrix)\n",
    "scores = networkx.pagerank(nx_graph)\n",
    "# 将相似度矩阵转为图结构\n",
    "ranked_sentences = sorted(\n",
    "    ((scores[i],s) for i,s in enumerate(sentences)),reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印得分最高的前10个句子，即为摘要\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KeyBERT in module keybert.model:\n",
      "\n",
      "class KeyBERT(builtins.object)\n",
      " |  KeyBERT(model: str = 'distilbert-base-nli-mean-tokens')\n",
      " |  \n",
      " |  A minimal method for keyword extraction with BERT\n",
      " |  \n",
      " |  The keyword extraction is done by finding the sub-phrases in\n",
      " |  a document that are the most similar to the document itself.\n",
      " |  \n",
      " |  First, document embeddings are extracted with BERT to get a\n",
      " |  document-level representation. Then, word embeddings are extracted\n",
      " |  for N-gram words/phrases. Finally, we use cosine similarity to find the\n",
      " |  words/phrases that are the most similar to the document.\n",
      " |  \n",
      " |  The most similar words could then be identified as the words that\n",
      " |  best describe the entire document.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      model: the name of the model used by sentence-transformer\n",
      " |             for a full overview see https://www.sbert.net/docs/pretrained_models.html\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: str = 'distilbert-base-nli-mean-tokens')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  extract_keywords(self, docs: Union[str, List[str]], keyphrase_ngram_range: Tuple[int, int] = (1, 1), stop_words: Union[str, List[str]] = 'english', top_n: int = 5, min_df: int = 1, use_maxsum: bool = False, use_mmr: bool = False, diversity: float = 0.5, nr_candidates: int = 20, vectorizer: sklearn.feature_extraction.text.CountVectorizer = None) -> Union[List[str], List[List[str]]]\n",
      " |      Extract keywords/keyphrases\n",
      " |      \n",
      " |      NOTE:\n",
      " |          I would advise you to iterate over single documents as they\n",
      " |          will need the least amount of memory. Even though this is slower,\n",
      " |          you are not likely to run into memory errors.\n",
      " |      \n",
      " |      Multiple Documents:\n",
      " |          There is an option to extract keywords for multiple documents\n",
      " |          that is faster than extraction for multiple single documents.\n",
      " |      \n",
      " |          However...this method assumes that you can keep the word embeddings\n",
      " |          for all words in the vocabulary in memory which might be troublesome.\n",
      " |      \n",
      " |          I would advise against using this option and simply iterating\n",
      " |          over documents instead if you have limited hardware.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          docs: The document(s) for which to extract keywords/keyphrases\n",
      " |          keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases\n",
      " |          stop_words: Stopwords to remove from the document\n",
      " |          top_n: Return the top n keywords/keyphrases\n",
      " |          min_df: Minimum document frequency of a word across all documents\n",
      " |                  if keywords for multiple documents need to be extracted\n",
      " |          use_maxsum: Whether to use Max Sum Similarity for the selection\n",
      " |                      of keywords/keyphrases\n",
      " |          use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the\n",
      " |                   selection of keywords/keyphrases\n",
      " |          diversity: The diversity of the results between 0 and 1 if use_mmr\n",
      " |                     is set to True\n",
      " |          nr_candidates: The number of candidates to consider if use_maxsum is\n",
      " |                         set to True\n",
      " |          vectorizer: Pass in your own CountVectorizer from scikit-learn\n",
      " |      \n",
      " |      Returns:\n",
      " |          keywords: the top n keywords for a document\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KeyBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
