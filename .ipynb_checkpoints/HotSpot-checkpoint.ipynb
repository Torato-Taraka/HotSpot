{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 遍历文档用的\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 提取关键词用的\n",
    "import yake\n",
    "# 将中文符号转换成英文符号\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词向量\n",
    "# 该词向量文件形式为：词 空格 词向量，然后换行\n",
    "# 从http://nlp.stanford.edu/data/glove.6B.zip获取GloVe\n",
    "word_embeddings = {}\n",
    "GLOVE_DIR = 'glove.840B.300d.txt'\n",
    "with open(GLOVE_DIR, encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "            word_embeddings[word] = coefs\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_print(text):\n",
    "    # 格式化输出，缩进4个单位\n",
    "    print(json.dumps(text, sort_keys = True, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_dir(dirname):\n",
    "    # 文档内容集合\n",
    "    text = []\n",
    "    # 遍历文档\n",
    "    for root, dirs, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            # print(file)\n",
    "            # 获取文件名\n",
    "            filename = os.path.splitext(file)[0]\n",
    "            # 读取文件\n",
    "            content = json.load(open(root + '/' + file, 'r', encoding = 'utf-8-sig'))\n",
    "            # 文件名添加到文件中，方便后续生成中间件\n",
    "            content['FileName'] = filename\n",
    "            content['Text'] = unicodedata.normalize('NFKC', content['Text'])\n",
    "            content['Text'] = content['Text'].lower()\n",
    "            content['Headline'] = unicodedata.normalize('NFKC', content['Headline'])\n",
    "            content['Headline'].replace(' – Manila Bulletin', '')\n",
    "            text.append(content)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Section</th>\n",
       "      <th>Writers</th>\n",
       "      <th>URL</th>\n",
       "      <th>MainKeyWord</th>\n",
       "      <th>AdditionalKeyWord</th>\n",
       "      <th>Source</th>\n",
       "      <th>FileName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>China says to have ‘prudent’ oil exploration w...</td>\n",
       "      <td>By ReutersChina will prudently advance coopera...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://mb.com.ph/2018/03/22/china-says-to-hav...</td>\n",
       "      <td>China</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>MB</td>\n",
       "      <td>0056d0f89b5c4e3f439700cd9ad227a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>PH to build more cellular sites on Pagasa Isla...</td>\n",
       "      <td>By Martin SadongdongAfter receiving several “w...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://mb.com.ph/2020/06/10/ph-to-build-more-...</td>\n",
       "      <td>China</td>\n",
       "      <td>viet Nam</td>\n",
       "      <td>MB</td>\n",
       "      <td>008661b734849123aea3b047872b56c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>March 30 coronavirus news</td>\n",
       "      <td>Our live coverage of the coronavirus pandemic ...</td>\n",
       "      <td>world</td>\n",
       "      <td>[Amy Woodyatt, Julia Hollingsworth, Ben Westco...</td>\n",
       "      <td>https://www.cnn.com/world/live-news/coronaviru...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>CNN</td>\n",
       "      <td>00969f304b601889e5e8c7ef6cc794e7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>Malaysia power shift hits China infrastructure...</td>\n",
       "      <td>By Agence France-PresseMalaysia was once a loy...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://mb.com.ph/2018/06/17/malaysia-power-sh...</td>\n",
       "      <td>China</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>MB</td>\n",
       "      <td>00a66477d6b91987e00affd4ca3f7ff9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-06-08</td>\n",
       "      <td>5 things to know for June 8: George Floyd, pol...</td>\n",
       "      <td>Feeling hopeless? Burnt out? You're not alone....</td>\n",
       "      <td>us</td>\n",
       "      <td>[AJ Willingham]</td>\n",
       "      <td>https://www.cnn.com/2020/06/08/us/five-things-...</td>\n",
       "      <td>China</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>CNN</td>\n",
       "      <td>00b596b867e63662c66ebfbb09da2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>May 4 coronavirus news</td>\n",
       "      <td>Our live coverage of the coronavirus pandemic ...</td>\n",
       "      <td>world</td>\n",
       "      <td>[Ben Westcott, Adam Renton]</td>\n",
       "      <td>https://www.cnn.com/world/live-news/coronaviru...</td>\n",
       "      <td>China</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>CNN</td>\n",
       "      <td>fe8157d39ff8c8d3cf3e248159a85b0d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Surgeries in Mexico linked to antibiotic-resis...</td>\n",
       "      <td>Nearly a dozen Americans who had surgery in Ti...</td>\n",
       "      <td>health</td>\n",
       "      <td>[Sandee LaMotte]</td>\n",
       "      <td>https://www.cnn.com/2019/01/10/health/mexico-s...</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>CNN</td>\n",
       "      <td>fef8577dfb7dd78f1e0d0d6c150c0fec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>Philippines journalist Maria Ressa found guilt...</td>\n",
       "      <td>Embattled Philippines journalist Maria Ressa w...</td>\n",
       "      <td>media</td>\n",
       "      <td>[James Griffiths]</td>\n",
       "      <td>https://www.cnn.com/2020/06/14/asia/maria-ress...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>CNN</td>\n",
       "      <td>ff06c9d4421059c3bb27b330fc70d88d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>Jack Ma's Ant Group was the next big thing. No...</td>\n",
       "      <td>Jack Ma's Ant Group quickly became one of Chin...</td>\n",
       "      <td>tech</td>\n",
       "      <td>[Laura He]</td>\n",
       "      <td>https://www.cnn.com/2021/02/19/tech/ant-group-...</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>CNN</td>\n",
       "      <td>ff5e31eed953acf427da23e7929ee72d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>2019-06-22</td>\n",
       "      <td>Southeast Asian leaders throw weight behind Ch...</td>\n",
       "      <td>By Agence France-PresseSoutheast Asian leaders...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://mb.com.ph/2019/06/22/southeast-asian-l...</td>\n",
       "      <td>China</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>MB</td>\n",
       "      <td>ffd73a78a0e1bf2106c869866106f1d0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>983 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time                                           Headline  \\\n",
       "0    2018-03-22  China says to have ‘prudent’ oil exploration w...   \n",
       "1    2020-06-10  PH to build more cellular sites on Pagasa Isla...   \n",
       "2    2020-03-30                          March 30 coronavirus news   \n",
       "3    2018-06-17  Malaysia power shift hits China infrastructure...   \n",
       "4    2020-06-08  5 things to know for June 8: George Floyd, pol...   \n",
       "..          ...                                                ...   \n",
       "978  2020-05-04                             May 4 coronavirus news   \n",
       "979  2019-01-10  Surgeries in Mexico linked to antibiotic-resis...   \n",
       "980  2020-06-15  Philippines journalist Maria Ressa found guilt...   \n",
       "981  2021-02-19  Jack Ma's Ant Group was the next big thing. No...   \n",
       "982  2019-06-22  Southeast Asian leaders throw weight behind Ch...   \n",
       "\n",
       "                                                  Text Section  \\\n",
       "0    By ReutersChina will prudently advance coopera...           \n",
       "1    By Martin SadongdongAfter receiving several “w...           \n",
       "2    Our live coverage of the coronavirus pandemic ...   world   \n",
       "3    By Agence France-PresseMalaysia was once a loy...           \n",
       "4    Feeling hopeless? Burnt out? You're not alone....      us   \n",
       "..                                                 ...     ...   \n",
       "978  Our live coverage of the coronavirus pandemic ...   world   \n",
       "979  Nearly a dozen Americans who had surgery in Ti...  health   \n",
       "980  Embattled Philippines journalist Maria Ressa w...   media   \n",
       "981  Jack Ma's Ant Group quickly became one of Chin...    tech   \n",
       "982  By Agence France-PresseSoutheast Asian leaders...           \n",
       "\n",
       "                                               Writers  \\\n",
       "0                                                        \n",
       "1                                                        \n",
       "2    [Amy Woodyatt, Julia Hollingsworth, Ben Westco...   \n",
       "3                                                        \n",
       "4                                      [AJ Willingham]   \n",
       "..                                                 ...   \n",
       "978                        [Ben Westcott, Adam Renton]   \n",
       "979                                   [Sandee LaMotte]   \n",
       "980                                  [James Griffiths]   \n",
       "981                                         [Laura He]   \n",
       "982                                                      \n",
       "\n",
       "                                                   URL MainKeyWord  \\\n",
       "0    https://mb.com.ph/2018/03/22/china-says-to-hav...       China   \n",
       "1    https://mb.com.ph/2020/06/10/ph-to-build-more-...       China   \n",
       "2    https://www.cnn.com/world/live-news/coronaviru...   Hong Kong   \n",
       "3    https://mb.com.ph/2018/06/17/malaysia-power-sh...       China   \n",
       "4    https://www.cnn.com/2020/06/08/us/five-things-...       China   \n",
       "..                                                 ...         ...   \n",
       "978  https://www.cnn.com/world/live-news/coronaviru...       China   \n",
       "979  https://www.cnn.com/2019/01/10/health/mexico-s...      Taiwan   \n",
       "980  https://www.cnn.com/2020/06/14/asia/maria-ress...   Hong Kong   \n",
       "981  https://www.cnn.com/2021/02/19/tech/ant-group-...   Hong Kong   \n",
       "982  https://mb.com.ph/2019/06/22/southeast-asian-l...       China   \n",
       "\n",
       "    AdditionalKeyWord Source                          FileName  \n",
       "0         Philippines     MB  0056d0f89b5c4e3f439700cd9ad227a4  \n",
       "1            viet Nam     MB  008661b734849123aea3b047872b56c1  \n",
       "2         Philippines    CNN  00969f304b601889e5e8c7ef6cc794e7  \n",
       "3            Malaysia     MB  00a66477d6b91987e00affd4ca3f7ff9  \n",
       "4            Malaysia    CNN  00b596b867e63662c66ebfbb09da2020  \n",
       "..                ...    ...                               ...  \n",
       "978          Malaysia    CNN  fe8157d39ff8c8d3cf3e248159a85b0d  \n",
       "979          Thailand    CNN  fef8577dfb7dd78f1e0d0d6c150c0fec  \n",
       "980    Southeast Asia    CNN  ff06c9d4421059c3bb27b330fc70d88d  \n",
       "981         Singapore    CNN  ff5e31eed953acf427da23e7929ee72d  \n",
       "982    Southeast Asia     MB  ffd73a78a0e1bf2106c869866106f1d0  \n",
       "\n",
       "[983 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取新闻数据，json格式\n",
    "text = read_data_from_dir('news_1')\n",
    "data = pd.DataFrame(text)\n",
    "data.drop('Type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_it(text):\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 10\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每一篇的关键短语都提取出来\n",
    "def extract_key_phrases_from_doc(docs):\n",
    "    doc_phrases, phrases_list = [], []\n",
    "    for doc in docs:\n",
    "        key_phrases_dict = yake_it(doc['Text'])\n",
    "        key_phrases_list = []\n",
    "        for tur in key_phrases_dict:\n",
    "            key_phrases_list.append(tur[0])\n",
    "            phrases_list.append(tur[0])\n",
    "        doc_phrases.append(key_phrases_list)\n",
    "    return doc_phrases, phrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换出（短语，编号）的字典\n",
    "def list_to_dict(phrases):\n",
    "    phrases = list(set(phrases))\n",
    "    num_phrases = len(phrases)\n",
    "    phrases_dict = {}\n",
    "    for i in range(num_phrases):\n",
    "        phrases_dict[phrases[i]] = i\n",
    "    return phrases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_phrases, phrases_list = extract_key_phrases_from_doc(text[:50])\n",
    "phrases_dict = list_to_dict(phrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrases_dict), len(phrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['By', 'ReutersChina', 'prudently', 'advance', 'cooperation', 'Philippines', 'joint', 'oil', 'gas', 'exploration', 'South', 'China', 'Sea', 'Chinas', 'top', 'diplomat', 'State', 'Councillor', 'Wang', 'Yi', 'said', 'Wednesday', 'meeting', 'Philippine', 'counterpartAny', 'potential', 'deals', 'Manila', 'Beijing', 'energy', 'exploration', 'disputed', 'waterway', 'agreed', 'company', 'Chinese', 'government', 'senior', 'Philippine', 'official', 'said', 'earlier', 'monthChina', 'claims', 'South', 'China', 'Sea', 'key', 'trade', 'route', 'home', 'areas', 'believed', 'hold', 'large', 'quantities', 'oil', 'natural', 'gas', 'Along', 'China', 'parts', 'South', 'China', 'Sea', 'subject', 'competing', 'claims', 'Brunei', 'Malaysia', 'Taiwan', 'Vietnam', 'PhilippinesThe', 'two', 'countries', 'February', 'agreed', 'set', 'special', 'panel', 'work', 'jointly', 'explore', 'offshore', 'oil', 'gas', 'areas', 'sides', 'claim', 'without', 'needing', 'address', 'touchy', 'issue', 'sovereigntySpeaking', 'reporters', 'Beijing', 'meeting', 'Philippine', 'Foreign', 'Affairs', 'Secretary', 'Alan', 'Peter', 'Cayetano', 'Wang', 'said', 'South', 'China', 'Sea', 'turned', 'source', 'friendship', 'cooperationWe', 'enhance', 'maritime', 'dialogue', 'pursue', 'equal', 'footed', 'friendly', 'consultation', 'prudent', 'steady', 'way', 'advance', 'cooperation', 'offshore', 'oil', 'gas', 'exploration', 'Wang', 'said', 'without', 'giving', 'detailsHowever', 'pursuing', 'joint', 'project', 'would', 'extremely', 'complex', 'sensitive', 'sharing', 'oil', 'gas', 'reserves', 'could', 'seen', 'endorsing', 'countries', 'claimsThe', 'Philippines', 'China', 'finding', 'common', 'legal', 'framework', 'conduct', 'joint', 'exploration', 'surveys', 'And', 'discussions', 'today', 'Im', 'confident', 'find', 'suitable', 'legal', 'framework', 'differences', 'Cayetano', 'saidPresident', 'Rodrigo', 'Duterte', 'said', 'China', 'proposed', 'joint', 'exploration', 'like', 'coownership', 'better', 'two', 'fighting', 'itThe', 'Philippines', 'suspended', 'exploration', 'Reed', 'Bank', '2014', 'pursue', 'legal', 'challenge', 'Chinas', 'territorial', 'claimsIncluded', '2016', 'ruling', 'Permanent', 'Court', 'Arbitration', 'Hague', 'clarification', 'Manilas', 'sovereign', 'right', 'access', 'offshore', 'oil', 'gas', 'fields', 'including', 'Reed', 'Bank', 'within', '200', 'mile', 'Exclusive', 'Economic', 'ZoneThe', 'Philippines', 'Chinas', 'CNOOC', 'Ltd', '0883HK', 'stateowned', 'Petro', 'Vietnam', 'jointly', 'surveyed', 'Reed', 'Bank', '2003', '2008The', 'Philippines', 'ties', 'China', 'warmed', 'Duterte', 'put', 'aside', 'territorial', 'disputes', 'exchange', 'trade', 'opportunities', 'pledged', 'financing', 'infrastructure', 'projectsCayetano', 'said', 'Duterte', 'would', 'visit', 'China', 'next', 'month', 'attend', 'economic', 'forum', 'southern', 'island', 'province', 'Hainan', 'meet', 'Chinese', 'President', 'Xi', 'Jinping']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "raw_sentences = text[0]['Text']\n",
    "raw_sentences=re.sub('[^\\w ]','',raw_sentences)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = word_tokenize(raw_sentences)\n",
    "filtered_sentence = [w for w in sentences if not w in stop_words]\n",
    "print(filtered_sentence)\n",
    "model = word2vec.Word2Vec([filtered_sentence], min_count=0, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Im', 0.14432227611541748),\n",
       " ('would', 0.13186022639274597),\n",
       " ('quantities', 0.12898240983486176),\n",
       " ('endorsing', 0.12429551035165787),\n",
       " ('official', 0.11843901872634888),\n",
       " ('access', 0.10748998075723648),\n",
       " ('joint', 0.10287690162658691),\n",
       " ('key', 0.10220304876565933),\n",
       " ('areas', 0.09904170781373978),\n",
       " ('ruling', 0.0968741774559021)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('advance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算共现矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5497, 5497)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算共现\n",
    "num_phrases = len(phrases_dict)\n",
    "co_occurance = np.array([[0] * num_phrases for _ in range(num_phrases)])\n",
    "for doc in doc_phrases:\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j:\n",
    "                co_occurance[phrases_dict[i]][phrases_dict[j]] += 1\n",
    "                co_occurance[phrases_dict[j]][phrases_dict[i]] += 1\n",
    "co_occurance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算相似度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_glove(phrase):\n",
    "    words = phrase.split()\n",
    "    vector = np.array([0.0 for _ in range(len(word_embeddings['hello']))], dtype = 'float32')\n",
    "    bias = 0\n",
    "    for word in words:\n",
    "        if word not in word_embeddings:\n",
    "            bias += 1\n",
    "            continue\n",
    "        vector += word_embeddings[word]\n",
    "    if bias == len(words):\n",
    "        return vector\n",
    "    return vector / (len(words) - bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 余弦相似度\n",
    "def CosineSimilarity(x, y):\n",
    "    sqrt_x, sqrt_y = np.sqrt((x ** 2).sum()), np.sqrt((y ** 2).sum())\n",
    "    if sqrt_x == 0 or sqrt_y == 0:\n",
    "        return 0.0\n",
    "    return (x * y).sum() / (sqrt_x * sqrt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_glove = np.zeros((num_phrases, num_phrases))\n",
    "for i in phrases_list:\n",
    "    for j in phrases_list:\n",
    "        if i != j:\n",
    "            simi = CosineSimilarity(phrase_glove(i), phrase_glove(j))\n",
    "            sim_glove[phrases_dict[i]][phrases_dict[j]] = simi\n",
    "            sim_glove[phrases_dict[j]][phrases_dict[i]] = simi\n",
    "sim_glove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算最终相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终相似度\n",
    "sim_final = np.zeros((num_phrases, num_phrases))\n",
    "# 计算共现的时候重复统计了\n",
    "co = np.array(co_occurance)\n",
    "co = normalization(co/2)\n",
    "for i in phrases_list:\n",
    "    for j in phrases_list:\n",
    "        if i != j:\n",
    "            simi = CosineSimilarity(co[phrases_dict[i]], co[phrases_dict[j]])\n",
    "            sim_final[phrases_dict[i]][phrases_dict[j]] = simi\n",
    "            sim_final[phrases_dict[j]][phrases_dict[i]] = simi\n",
    "sim_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "sim_final = sim_final * alpha + sim_glove * (1 - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectralClustering聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "n_clusters, ch_max = 0, 0.0\n",
    "\n",
    "for num in range(10, 20):\n",
    "    cluster = SpectralClustering(n_clusters=num, affinity='nearest_neighbors', n_neighbors=5).fit_predict(sim_final)\n",
    "    CH = metrics.calinski_harabasz_score(sim_final, cluster)\n",
    "    if CH > ch_max:\n",
    "        ch_max = CH\n",
    "        n_clusters = num\n",
    "cluster = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', n_neighbors=5).fit_predict(sim_final)\n",
    "cluster, n_clusters, CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = pd.DataFrame.from_dict(phrases_dict, orient='index', columns=['id'])\n",
    "phrases['clusters'] = cluster\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases.loc['hate'].clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(doc_phrases).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cluster = [[] for _ in range(n_clusters)]\n",
    "for i in range(50):\n",
    "    for j in doc_phrases[i]:\n",
    "        if i not in article_cluster[phrases.loc[j].clusters]:\n",
    "            article_cluster[phrases.loc[j].clusters].append(i)\n",
    "article_cluster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sumy文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本摘要\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "\n",
    "#设置输出句子总数\n",
    "SENTENCES_COUNT = 10\n",
    "\n",
    "def plainTextSummary(data,language):\n",
    "    \"\"\"\n",
    "    基于明文数据内容的摘要方法\n",
    "    \"\"\"\n",
    "    \n",
    "    parser = PlaintextParser.from_string(data, Tokenizer(language))\n",
    "    stemmer = Stemmer(language)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "\n",
    "data_to_sumy = list(['. '.join(x for x in list(data.iloc[article_cluster[0]].Headline))])\n",
    "\n",
    "plainTextSummary(data_to_sumy,'english')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[article_cluster[9]].Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases.loc[phrases.clusters == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'. '.join(x for x in list(data.iloc[article_cluster[0]].Headline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "result = summarize('. '.join(x for x in list(data.iloc[article_cluster[0]].Headline)), split=True, word_count=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "help(gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from pre_trained.model import Model\n",
    "\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "\n",
    "\n",
    "class SumGen:\n",
    "    def __init__(self, model_dir=\".\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_dir: 模型文件的保存路径, 需要包含从pre_trained中解压出来的文件\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir\n",
    "        with Path(model_dir, \"pre_trained/args.pickle\").open(\"rb\") as f:\n",
    "            self.args = pickle.load(f)\n",
    "\n",
    "        print(\"Loading dictionary...\")\n",
    "        with Path(model_dir, \"pre_trained/word_dict.pickle\").open(\"rb\") as f:\n",
    "            self.word_dict = pickle.load(f)\n",
    "        self.reversed_dict = dict(zip(self.word_dict.values(), self.word_dict.keys()))\n",
    "        self.article_max_len = 50\n",
    "        self.summary_max_len = 15\n",
    "        \n",
    "    def build_dataset(self, raw_articles: list) -> list:\n",
    "        \"\"\"生成数据集\n",
    "\n",
    "        Args:\n",
    "            raw_articles: 一个列表, 包含所有的文档\n",
    "        \"\"\"\n",
    "        article_list = [clean_str(x.strip()) for x in raw_articles]\n",
    "        x = [word_tokenize(d) for d in article_list]\n",
    "        x = [[self.word_dict.get(w, self.word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "        x = [d[:self.article_max_len] for d in x]\n",
    "        x = [d + (self.article_max_len - len(d)) * [self.word_dict[\"<padding>\"]] for d in x]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def generate(self, valid_x: list) -> list:\n",
    "        \"\"\"生成文本摘要\n",
    "\n",
    "        Args:\n",
    "            valid_x: 用build_dataset生成的数据集\n",
    "\n",
    "        Returns:\n",
    "            返回一个列表, 包含每一个样本生成的标题\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        with tf.Session() as sess:\n",
    "            # 加载模型\n",
    "            print(\"Loading saved model...\")\n",
    "            model = Model(self.reversed_dict, self.article_max_len, self.summary_max_len, self.args, forward_only=True)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            ckpt = tf.train.get_checkpoint_state(Path(self.model_dir, \"pre_trained/saved_model/\").as_posix())\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            batches = batch_iter(valid_x, [0] * len(valid_x), self.args.batch_size, 1)\n",
    "\n",
    "            print(\"Generating...\")\n",
    "            for batch_x, _ in batches:\n",
    "                batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "                valid_feed_dict = {\n",
    "                    model.batch_size: len(batch_x),\n",
    "                    model.X: batch_x,\n",
    "                    model.X_len: batch_x_len,\n",
    "                }\n",
    "\n",
    "                prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "                prediction_output = [[self.reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    results.append(\" \".join(summary))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['February 4 coronavirus news. November 1 coronavirus news. July 5 coronavirus news.']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sg = SumGen()\n",
    "valid_x = sg.build_dataset(dataset)\n",
    "sg.generate(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 热度指数\n",
    "\n",
    "考虑时间差，同一类文章数，同一类文章中不同的来源数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cluster = np.array(article_cluster)\n",
    "article_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date = '2019-01-10'\n",
    "new_date = datetime.datetime.strptime(new_date, '%Y-%m-%d')\n",
    "new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date - new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    data.iloc[i].Time = datetime.datetime.strptime(data.iloc[i].Time, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = []\n",
    "for i in range(n_clusters):\n",
    "    delta = date - data.iloc[article_cluster[i]].Time\n",
    "    time = []\n",
    "    for j in range(len(delta)):\n",
    "        time.append(delta.iloc[j].days)\n",
    "    time = np.array(time).mean()\n",
    "    timestamp.append(time)\n",
    "timestamp = np.array(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(len(article_cluster[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.Source == 'CNN'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(data.Source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(x, y):\n",
    "    return np.log(1 + x) / np.log(1 + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.1\n",
    "def time_score(i):\n",
    "    return 2 /(1 + np.power(timestamp[i], alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 0\n",
    "for i in range(n_clusters):\n",
    "    if len(article_cluster[i]) > max_n: max_n = len(article_cluster[i])\n",
    "max_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_score(i):\n",
    "    return compute_score(len(article_cluster[i]), max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_score_1(i):\n",
    "    return 1 / np.log(1 + len(set(data.Source)) / len(set(data.iloc[article_cluster[i]].Source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_score_2(i):\n",
    "    return compute_score((len(set(data.iloc[article_cluster[i]].Source))), np.exp(len(set(data.Source))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotscore = np.zeros(n_clusters)\n",
    "alpha_t, alpha_n, alpha_s = 0.4, 0.5, 0.1\n",
    "for i in range(n_clusters):\n",
    "    hotscore[i] = (alpha_t * time_score(i) + alpha_n * number_score(i) + alpha_s * source_score_2(i)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_score(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(data.Source)) / len(set(data.iloc[article_cluster[10]].Source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / np.power(2, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1 + 1) / np.log(1 + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotspot",
   "language": "python",
   "name": "hotspot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
